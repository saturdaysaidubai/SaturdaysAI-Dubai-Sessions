{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AISaturdays Rental Challenge**\n",
    "\n",
    "![AISaturdays](https://www.saturdays.ai/assets/images/ai-saturdays-122x122.png)\n",
    "\n",
    "Welcome to the challenge of **AISaturdays** of Artificial Intelligence for the prediction of rental prices of the neighborhoods of a city. In this exercise we will estimate the price of a rental offer, depending on the data described below.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- The Python 3 programming language will be used.\n",
    "- The python libraries will be used: Pandas, MatPlotLib, Numpy.\n",
    "\n",
    "**Through this exercise, we will learn:**\n",
    "- Understand and run NoteBooks with Python.\n",
    "- Being able to use Python functions and additional libraries.\n",
    "- Dataset:\n",
    " - Get the dataset and preview the dataset information.\n",
    " - Clean and normalize the dataset information.\n",
    " - Represent and analyze the dataset information.\n",
    "- Apply the Random Forest algorithm\n",
    "- Improve prediction using Hyperparameter Tunning, Feature engineering and Gradient Boosting\n",
    "\n",
    "Let us begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.Import of libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Read the .csv with the data and show the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shows the number of features and examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get what datatypes (dtypes) the dataset contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "\n",
    "\n",
    "* **Id/name:**  Identifier and name of the offer.\n",
    "\n",
    "* **host_id/host_name:** Identifier and host name.\n",
    "\n",
    "* **neighbourhood_group/neighbourhood:** Zone and neighborhood of the offer. Each zone is a grouping of neighborhoods.\n",
    "* **latitude/longitude:** Latitude and longitude of the offer.\n",
    "\n",
    "* **room_type:** What type of room is offered. It can be the whole apartment or house, a private room or a shared one.\n",
    "\n",
    "* **minimum_nights:**  Minimum stay nights.\n",
    "\n",
    "* **number_of_reviews:**  Total number of reviews of the offer.\n",
    "\n",
    "* **last_review:**  Date of the last review made.\n",
    "\n",
    "* **reviews_per_month:** Number of reviews per month. It is not always integer and most are less than 1.\n",
    "\n",
    "* **calculated_host_listings_count:** How many rooms does the host have on offer?\n",
    "\n",
    "* **availability_365:** The availability of the offer in one year: maximum of 365 (all year on offer)\n",
    "\n",
    "* **price:** Our objetive!. The price of the offer, in dollars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a regression or classification problem? Why?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Before parsing the dataset, we have to transform the dates (the last_review feature) into something we can work with. Pandas has a data type specifically for this, datetime. Transform last_review to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only with one line of code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To analyze the data we also need to know how much information we lack. Use isnull () to find out which feature is missing more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only with one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we only need to get rid of the features that only serve as an identifier and do not help to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only with one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. All ready! We can now analyze the distribution of the data with the function .describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only with one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and normalize dataset information\n",
    "![texto alternativo](https://i.imgur.com/8u4xTI7.png)\n",
    "\n",
    "This dataset contains incomplete information that we must fill in to be able to use it to predict the price of the offers..\n",
    "We also have to transform last_review if we want to include it in the prediction (we cannot use a date as input directly).\n",
    "\n",
    "For this cleaning we will use various functions of Pandas. Here's a [hint](https://new.paradigmadigital.com/wp-content/uploads/2019/02/Pandas_cheatsheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Find the number of offers that due to not having reviews do not have information in the columns of last_review and reviews_per_month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only with one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We have to fill in this information if we don't want to delete the rest of the example. Fill all the NaNs of the reviews_per_month with 0 (We will complete the last_review column later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only with one line of code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We are going to transform the last_review variable. It is a date, which makes it difficult for us to work with it. Let's first complete the examples that don't have a last date. Replace these NaNs with the first historical review of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now that we don't have empty values ​​we can change the last_review variable to something more useful. We seek that smaller values ​​correspond to older reviews or that have not had any, while larger values ​​correspond to more recent reviews. \n",
    "\n",
    "We can use the toordinal () function to find the number of days that have passed since day 1 of year 1, but those are still too large numbers that do not follow the distribution we are looking for.\n",
    "\n",
    "Get last_reviews to represent the number of days that have passed since the first historical review was made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. To visualize the distribution of dates, generate a graph showing the variable last_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two very distinguished groups. What is this distribution due to ?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of the variable to predict and noise elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. When it comes to predicting the price, it is much more favorable if we first transform and analyze the variable we are looking for to make it easier to predict.\n",
    "\n",
    "First, let's see how the price of the offers is distributed. Generate a graph showing the price of the offers. Here's a [Hint](https://seaborn.pydata.org/generated/seaborn.distplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just one line of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a variable that follows a log-normal distribution. We can transform it into a normal distribution by applying log1p (), a function that responds to the following equation:\n",
    "\n",
    "$ y = log(x+1) $\n",
    "\n",
    "This makes the price easier to predict, having a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. We are going to visualize this transformation. Generate another graph of price after applying the log1p () function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a much more appropriate distribution for making predictions. However, there are still many outliers that add noise to the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Above and below what values ​​is this noise present? Removes from the dataframe those values ​​that do not fall within the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Now, rebuild the price chart and log1p of the price (use the same code as before, or put it in a [subplot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Four lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Finally, we have a normalized, noise-free output variable that will improve our predictions. Change the variable price to the log1p of price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a little more the rest of variables that can affect the price of an offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Let's start by creating a histogram of the different areas of the city and the number of offers in each of them (maybe you need to enlarge the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Now create a map of the offered apartments with the latitude and longitude (extra points if you color them by zones or neighborhoods). It is best to do it in a subplot so you can control the size of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Now we are going to generate another histogram, this time with the offered room type (It is also a good idea to adjust the size of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable transformation We can apply the same process that we apply to the variable price to our input variables and thus achieve a more comfortable distribution for the search methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "19. Apply the log1p () transformation to minimum_nights, generating the before and after graphs and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Finally save minimum_nights as log1p of minimum_nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Repeat the process, this time with reviews_per_month. Is the transformation relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability study in number of days(0,365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Let's start by representing the availability in a distplot (). Since we know the limits of this variable, it is best to limit the range of the graph and make it larger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#four lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add artificial variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been seen in the previous scatterplot that there appear to be two groups, one available most of the year and the other only a few days.\n",
    "\n",
    "It is also intuited that those sites that do not have reviews ... As they do not give much confidence? ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Add three categories that measure if the apartment is available all year round, if its availability is very low (less than 12 days a year), and if it has no reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. We are going to generate a heatmap that shows the relationship between all the input and price variables. Use corr () and seaborn's heatmap () function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass categorical variables to one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. To make the categorical features easier to interpret by the model, we are going to transform them into a OneHotEncoding. Use pandas get_dummies () function (you should have 241 columns left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models, models, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the data exploration, analysis and cleaning done, we move on to the fun part: The Models!\n",
    "    \n",
    "We start by importing all the classes that we will need to find a good predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score,  GridSearchCV, KFold, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Divide the dataset into X_train, X_test, y_train and y_test using train_test_split (). Don't forget not to include a price in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. We are going to use cross_validation to train our model, using Kfold to find the score. Implement a Kfold that performs 5 splits and calculates the mean error and deviation of a RandomForestRegressor without changing its parameters (yet). [Hint](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. When using a RandomForestRegressor, what hyperparameters were we using? List all the parameters that this model uses (uses the get_params () function and the pprint library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust all these parameters to improve the accuracy of our model. One way to find which combination works best is to use a GridSearchCV, which tests models with many different combinations and calculates your score to find the best model in brute force. For this, you have to pass a list of values ​​for each parameter, and GridSearchCV will test them all. [More information](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. Delimit what values ​​you want each parameter to have, and include each of these lists in a dictionary to be able to run the GridSearchCV. Consider the possible values ​​for each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. We can now implement a GridSearchCV. To make it faster, a version is used that does not test with all possible combinations, but with a few random ones (hence its name, RandomizedSearchCV). Implement it, taking into account that it has as parameters the model to adjust and the dictionary that we have defined before, among others. This step may take a few minutes as you have to adjust many models to find the best one. Here is the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) del RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. To finish, find the mean squared error and$R^2$ the best model you can find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Six lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to improve that score!\n",
    "You can try:\n",
    "- Delete features that are not relevant to prediction\n",
    "- Implement Gradient boosting using XBoost or Adaboost, among others\n",
    "- Adjust hyperparameters manually to get better models\n",
    "- Use a Tree Interpreter to see which decision trees are most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the challenge, we will give you a validation set to see which group has achieved the best score. Whoever wins has a prize!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
