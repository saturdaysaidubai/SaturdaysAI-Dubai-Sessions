{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushroom Dataset\n",
    "\n",
    "You can get the data set at the following link:\n",
    "\n",
    "[Mushroom Dataset](https://www.kaggle.com/uciml/mushroom-classification)\n",
    "\n",
    "As you can see, there are many variables, all of them categorical, so scatterplot explorations will not be useful to us as in other cases.\n",
    "\n",
    "The variable to predict ``class`` it is categorical, so there will be no need to rescale with functions like the logarithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load of libraries, which we have considered basic, add what you want :)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset and first look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the csv and see a siple head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset description, standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on the data type of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the number of nulls of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# like other times, one line, count nulls per variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for strange values. To do this, see the different values ​​in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a new dataframe where a column is the no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat those values ​​that we understand to be null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocations. You can remove those points, impute with fashion or leave that value as one more possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look how many values ​​are in each feature, do all the features provide information? If any does not provide information, delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave by the wayside if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate between predictor variables and variables to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable that this dataset tries to predict is class\n",
    "y = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly encode categorical to numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot, it's a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split, duh;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We leave it to everyone the same :)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a dataset that we have not seen anything about yet (there are no drawings) so we are going to do some of them. We have the problem that there are many variables ... Well, PCA to the rescue : We ask you to give us two dimensions and we paint them, we know that they will be ** those that retain more information **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca =       # sklearn method \n",
    "pca.fit(X_train)\n",
    "\n",
    "# render on a scatterplot and color the training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that it is quite separate, it seems that much can be seen by eye :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, and to avoid cases such as that of the quality of the wine;), we will train a classifier to see how it does before editing more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the classifier and the number of estimators\n",
    "# train train\n",
    "# calculate precision on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh sh * t, it turns out he nails it !! Go back up, check that you have correctly used the predictor variable, run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naaaah, that's a joke, that's fine, it's a simple set and Random Forest is very good at its work xD Likewise, let's see what size of dataset we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ufffffff A lot of features, right? We will reduce to see how it is. How? Come on, look at what the section is called, it will surely give you a clue;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = # define a range of values to test\n",
    "scores = []\n",
    "\n",
    "for n in n_features:\n",
    "    \n",
    "    # Make PCA on X_train\n",
    "    # 1- Define PCA\n",
    "    # 2- Learn PCA about X_train\n",
    "    \n",
    "    # Train Random Forest\n",
    "    # 1- Define the RF\n",
    "    # 2- Train classifier\n",
    "    \n",
    "    # Save the score\n",
    "    \n",
    "    \n",
    "sns.lineplot(x=n_features, y=scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are seeing that from about 10 features we already have the score we wanted and we have also reduced the variables to 10% of those we had, even less than the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that the data set is simple, we can try to do some clustering to see what information we can obtain.\n",
    "\n",
    "The first step is going to be importing the klear function from sklearn, and from there, we are going to look for the optimal value of clusters. As we have seen previously, this value is obtained, for example, from the elbow of the graph that represents the total distances from the points to the centers of the associated clusters. I leave the sklearn documentation page for you to look for it:\n",
    "\n",
    "[K-Means on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "With this you only have to now generate the kmean models, evaluate and paint the graph for the values ​​of ``k`` that you establish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scores = []\n",
    "k_values = # define a range\n",
    "for a in k_values:\n",
    "    \n",
    "    # Define Kmeans and adjust\n",
    "    # Save the prediction\n",
    "        \n",
    "sns.lineplot(x=k_values, y=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the value you have obtained from the graph, you can get a good approximation of Kmeans and with this we can go on to explore how well the different clusters have separated the information. To do this, a ``factorplot``, seaborn will do it alone. With this, what we want to see is the distribution of the varaible to be predicted based on the cluster determined by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learn Kmeans with the K value obtained\n",
    "\n",
    "kmeans = # Define and train Kmeans\n",
    "\n",
    "# Prepare the factorplot\n",
    "\n",
    "\n",
    "# Paint\n",
    "ax = sns.factorplot(col=, x=, data=, kind='count',col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this is painted. For this, we repeat the scatterplot from before but using as a color the cluster assigned by kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train PCA to represent\n",
    "\n",
    "# Represent the clusters as color \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite similar isn't it? As good as the wonderful Random Forest is not, but it has managed to identify the different points of the dataset quite well. In fact, the factor diagram that we have seen before shows that only a couple of clusters are imprecise, the rest in case we get the variable to predict already clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
